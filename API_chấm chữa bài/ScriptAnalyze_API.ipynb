{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VK0jReW9G7J1",
        "outputId": "a2858dbe-942d-4e2a-ac5f-6e731a18e39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-14' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: praat-parselmouth in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (24.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.45.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: cmudict in /usr/local/lib/python3.11/dist-packages (from textstat) (1.0.32)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn transformers torch spacy language-tool-python nltk scikit-learn praat-parselmouth textstat sentence-transformers pyngrok pydantic nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vjf45YXbbeiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok pydantic"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY7VW_CNKbyR",
        "outputId": "df966ddc-23ad-4f18-abbc-6e7a8304e2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEmDn19LMwpP",
        "outputId": "67d913f8-6af2-42c1-9701-cfc6058adbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import language_tool_python\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "V9vvNT-KK3sp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import language_tool_python\n",
        "import textstat\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Load mô hình spaCy để nhận diện Candidate & Examiner\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load BERT để phân loại câu trả lời\n",
        "classifier = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-SST-2\")\n",
        "\n",
        "# Load công cụ kiểm tra ngữ pháp\n",
        "grammar_tool = language_tool_python.LanguageTool('en-UK')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QltT1p55Ibyr",
        "outputId": "a622d9cc-deff-4ea5-b131-73320121d488"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hàm chuẩn hóa dữ liệu"
      ],
      "metadata": {
        "id": "yWq_h-wKK7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_transcription(transcription):\n",
        "    # Tách các phần dựa theo mẫu '--- Part X ---'\n",
        "    parts = re.split(r'--- Part (\\d+) ---', transcription)\n",
        "    result = []  # List chứa các list con, mỗi list con có dạng: [part, question, response]\n",
        "\n",
        "    # Bắt đầu từ index 1, duyệt theo cặp: số part và nội dung của part\n",
        "    for i in range(1, len(parts), 2):\n",
        "        try:\n",
        "            part_number = int(parts[i].strip())\n",
        "        except ValueError:\n",
        "            continue\n",
        "        part_text = parts[i + 1].strip()\n",
        "\n",
        "        # Tìm kiếm các cặp câu hỏi - câu trả lời\n",
        "        pattern = re.compile(r'examiner: (.*?)\\n(candidate: .*?)(?=\\nexaminer:|\\n--- Part|\\Z)', re.DOTALL)\n",
        "        pairs = pattern.findall(part_text)\n",
        "\n",
        "        for question, response in pairs:\n",
        "            # Loại bỏ tiền tố \"candidate: \" trong câu trả lời\n",
        "            response_clean = response.replace(\"candidate: \", \"\").strip()\n",
        "            # Append kết quả dưới dạng list con: [part, question, response]\n",
        "            result.append([part_number, question.strip(), response_clean])\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "_Z7RmYyZIhbr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hàm xem độ tương quan của câu trả lời"
      ],
      "metadata": {
        "id": "t4DtdF29Nme-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_answer_relevance(data_list):\n",
        "    try:\n",
        "        # Initialize the model\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Print header\n",
        "        print(f\"Current Date and Time (UTC - YYYY-MM-DD HH:MM:SS formatted): {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Current User's Login: NgPcAnhh\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        def evaluate_relevance(question, answer):\n",
        "            \"\"\"Calculate cosine similarity between question and answer embeddings.\"\"\"\n",
        "            q_embedding = model.encode(question, convert_to_tensor=True)\n",
        "            a_embedding = model.encode(answer, convert_to_tensor=True)\n",
        "\n",
        "            similarity = util.cos_sim(q_embedding, a_embedding).item()\n",
        "            similarity_percentage = similarity * 100\n",
        "\n",
        "            if similarity < 0.2:\n",
        "                label = \"Not Relevant\"\n",
        "            elif similarity < 0.5:\n",
        "                label = \"Neutral\"\n",
        "            else:\n",
        "                label = \"Relevant\"\n",
        "\n",
        "            return similarity_percentage, label\n",
        "\n",
        "        # Analyze each entry\n",
        "        for entry in data_list:\n",
        "            part_number = entry[0]\n",
        "            question = entry[1]\n",
        "            answer = entry[2]\n",
        "\n",
        "            # Calculate relevance\n",
        "            relevance_score, relevance_label = evaluate_relevance(question, answer)\n",
        "\n",
        "            # Print analysis for this entry\n",
        "            print(f\"\\nPart {part_number}\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f\"Relevance Score: {relevance_score:.2f}%\")\n",
        "            print(f\"Label: {relevance_label}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during analysis: {str(e)}\")"
      ],
      "metadata": {
        "id": "lcX6Hnv-OVbW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hàm check lỗi ngữ pháp"
      ],
      "metadata": {
        "id": "Bb7ZiYDJOZTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_all_answers(data_list):\n",
        "    try:\n",
        "        # Initialize the grammar tool\n",
        "        grammar_tool = language_tool_python.LanguageTool('en-UK')\n",
        "\n",
        "        print(f\"\\n====== Grammar Analysis Report ======\")\n",
        "        print(f\"Analysis Time: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Total entries to analyze: {len(data_list)}\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Dictionary to store all errors\n",
        "        all_results = {}\n",
        "\n",
        "        # Analyze each entry in the data list\n",
        "        for entry in data_list:\n",
        "            part_number = entry[0]\n",
        "            question = entry[1]\n",
        "            answer_text = entry[2]  # Last element is always the answer\n",
        "\n",
        "            print(f\"\\n\\n--- Part {part_number} ---\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"\\nAnswer: {answer_text}\")\n",
        "            print(\"\\nGrammar Analysis:\")\n",
        "\n",
        "            # Get all matches (errors)\n",
        "            matches = grammar_tool.check(answer_text)\n",
        "\n",
        "            # Filter out punctuation errors\n",
        "            excluded_rules = [\n",
        "                \"COMMA_COMPOUND_SENTENCE\",\n",
        "                \"COMMA_PARENTHESIS_WHITESPACE\",\n",
        "                \"PUNCTUATION_COMMA\",\n",
        "                \"PUNCTUATION_PERIOD\",\n",
        "                \"PUNCTUATION_QUOTATION\",\n",
        "                \"PUNCTUATION_WHITESPACE\",\n",
        "                \"COMMA_PARENTHESIS_START_WHITESPACE\",\n",
        "                \"COLON_WHITESPACE\",\n",
        "                \"EXTRA_COMMA\",\n",
        "                \"MISSING_COMMA\",\n",
        "                \"SEMICOLON_WHITESPACE\",\n",
        "                \"EXTRA_PERIOD\",\n",
        "                \"MISSING_PERIOD\",\n",
        "                \"EXTRA_WHITESPACE\",\n",
        "                \"MISSING_WHITESPACE\",\n",
        "                \"WHITESPACE_RULE\"\n",
        "            ]\n",
        "\n",
        "            # Create markers for error positions\n",
        "            error_markers = list(\" \" * len(answer_text))\n",
        "            errors_found = []\n",
        "\n",
        "            for match in matches:\n",
        "                if not match.ruleId.startswith(\"PUNCT\") and match.ruleId not in excluded_rules:\n",
        "                    # Mark error position\n",
        "                    for pos in range(match.offset, match.offset + match.errorLength):\n",
        "                        if pos < len(error_markers):\n",
        "                            error_markers[pos] = \"^\"\n",
        "\n",
        "                    error_info = {\n",
        "                        \"error_type\": match.ruleId,\n",
        "                        \"message\": match.message,\n",
        "                        \"context\": match.context,\n",
        "                        \"suggestion\": match.replacements[0] if match.replacements else \"No suggestion available\",\n",
        "                        \"position\": (match.offset, match.offset + match.errorLength),\n",
        "                        \"text\": answer_text[match.offset:match.offset + match.errorLength]\n",
        "                    }\n",
        "                    errors_found.append(error_info)\n",
        "\n",
        "            # Store results for this part\n",
        "            all_results[part_number] = {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer_text,\n",
        "                \"errors\": errors_found\n",
        "            }\n",
        "\n",
        "            # Print error details\n",
        "            if errors_found:\n",
        "                print(f\"\\nFound {len(errors_found)} errors:\")\n",
        "                for i, error in enumerate(errors_found, 1):\n",
        "                    print(f\"\\n{i}. Error Type: {error['error_type']}\")\n",
        "                    print(f\"   Found Text: \\\"{error['text']}\\\"\")\n",
        "                    print(f\"   Message: {error['message']}\")\n",
        "                    print(f\"   Suggestion: {error['suggestion']}\")\n",
        "\n",
        "                # Print text with error markers\n",
        "                print(\"\\nError Locations:\")\n",
        "                print(answer_text)\n",
        "                print(\"\".join(error_markers))\n",
        "            else:\n",
        "                print(\"No grammar errors found in this answer!\")\n",
        "\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        # Print overall summary\n",
        "        print(\"\\n====== Overall Summary ======\")\n",
        "        total_errors = sum(len(part_data[\"errors\"]) for part_data in all_results.values())\n",
        "        print(f\"Total parts analyzed: {len(data_list)}\")\n",
        "        print(f\"Total errors found: {total_errors}\")\n",
        "        print(f\"Average errors per part: {total_errors/len(data_list):.2f}\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during grammar analysis: {str(e)}\")\n",
        "        return {}\n",
        "\n"
      ],
      "metadata": {
        "id": "jbrcODkpOYrV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API"
      ],
      "metadata": {
        "id": "V3ozzrzBQjPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datetime import datetime\n",
        "import language_tool_python\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Mô tả dữ liệu đầu vào\n",
        "class TranscriptionInput(BaseModel):\n",
        "    transcription: list\n",
        "\n",
        "# Your provided function, unchanged\n",
        "def parse_transcription(transcription):\n",
        "    # Tách các phần dựa theo mẫu '--- Part X ---'\n",
        "    parts = re.split(r'--- Part (\\d+) ---', transcription)\n",
        "    result = []  # List chứa các list con, mỗi list con có dạng: [part, question, response]\n",
        "\n",
        "    # Bắt đầu từ index 1, duyệt theo cặp: số part và nội dung của part\n",
        "    for i in range(1, len(parts), 2):\n",
        "        try:\n",
        "            part_number = int(parts[i].strip())\n",
        "        except ValueError:\n",
        "            continue\n",
        "        part_text = parts[i + 1].strip()\n",
        "\n",
        "        # Tìm kiếm các cặp câu hỏi - câu trả lời\n",
        "        pattern = re.compile(r'examiner: (.*?)\\n(candidate: .*?)(?=\\nexaminer:|\\n--- Part|\\Z)', re.DOTALL)\n",
        "        pairs = pattern.findall(part_text)\n",
        "\n",
        "        for question, response in pairs:\n",
        "            # Loại bỏ tiền tố \"candidate: \" trong câu trả lời\n",
        "            response_clean = response.replace(\"candidate: \", \"\").strip()\n",
        "            # Append kết quả dưới dạng list con: [part, question, response]\n",
        "            result.append([part_number, question.strip(), response_clean])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Your provided function, modified to return the data instead of printing\n",
        "def analyze_answer_relevance(data_list):\n",
        "    try:\n",
        "        # Initialize the model\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Result list\n",
        "        results = []\n",
        "\n",
        "        def evaluate_relevance(question, answer):\n",
        "            \"\"\"Calculate cosine similarity between question and answer embeddings.\"\"\"\n",
        "            q_embedding = model.encode(question, convert_to_tensor=True)\n",
        "            a_embedding = model.encode(answer, convert_to_tensor=True)\n",
        "\n",
        "            similarity = util.cos_sim(q_embedding, a_embedding).item()\n",
        "            similarity_percentage = similarity * 100\n",
        "\n",
        "            if similarity < 0.2:\n",
        "                label = \"Not Relevant\"\n",
        "            elif similarity < 0.5:\n",
        "                label = \"Neutral\"\n",
        "            else:\n",
        "                label = \"Relevant\"\n",
        "\n",
        "            return similarity_percentage, label\n",
        "\n",
        "        # Analyze each entry\n",
        "        for entry in data_list:\n",
        "            part_number = entry[0]\n",
        "            question = entry[1]\n",
        "            answer = entry[2]\n",
        "\n",
        "            # Calculate relevance\n",
        "            relevance_score, relevance_label = evaluate_relevance(question, answer)\n",
        "\n",
        "            # Store analysis for this entry\n",
        "            results.append({\n",
        "                \"part_number\": part_number,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"relevance_score\": relevance_score,\n",
        "                \"label\": relevance_label\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Your provided function, modified to return the data instead of printing\n",
        "def analyze_all_answers(data_list):\n",
        "    try:\n",
        "        # Initialize the grammar tool\n",
        "        grammar_tool = language_tool_python.LanguageTool('en-UK')\n",
        "\n",
        "        # Dictionary to store all errors\n",
        "        all_results = {}\n",
        "\n",
        "        # Analyze each entry in the data list\n",
        "        for entry in data_list:\n",
        "            part_number = entry[0]\n",
        "            question = entry[1]\n",
        "            answer_text = entry[2]  # Last element is always the answer\n",
        "\n",
        "            # Get all matches (errors)\n",
        "            matches = grammar_tool.check(answer_text)\n",
        "\n",
        "            # Filter out punctuation errors\n",
        "            excluded_rules = [\n",
        "                \"COMMA_COMPOUND_SENTENCE\",\n",
        "                \"COMMA_PARENTHESIS_WHITESPACE\",\n",
        "                \"PUNCTUATION_COMMA\",\n",
        "                \"PUNCTUATION_PERIOD\",\n",
        "                \"PUNCTUATION_QUOTATION\",\n",
        "                \"PUNCTUATION_WHITESPACE\",\n",
        "                \"COMMA_PARENTHESIS_START_WHITESPACE\",\n",
        "                \"COLON_WHITESPACE\",\n",
        "                \"EXTRA_COMMA\",\n",
        "                \"MISSING_COMMA\",\n",
        "                \"SEMICOLON_WHITESPACE\",\n",
        "                \"EXTRA_PERIOD\",\n",
        "                \"MISSING_PERIOD\",\n",
        "                \"EXTRA_WHITESPACE\",\n",
        "                \"MISSING_WHITESPACE\",\n",
        "                \"WHITESPACE_RULE\"\n",
        "            ]\n",
        "\n",
        "            # Create markers for error positions\n",
        "            error_markers = list(\" \" * len(answer_text))\n",
        "            errors_found = []\n",
        "\n",
        "            for match in matches:\n",
        "                if not match.ruleId.startswith(\"PUNCT\") and match.ruleId not in excluded_rules:\n",
        "                    # Mark error position\n",
        "                    for pos in range(match.offset, match.offset + match.errorLength):\n",
        "                        if pos < len(error_markers):\n",
        "                            error_markers[pos] = \"^\"\n",
        "\n",
        "                    error_info = {\n",
        "                        \"error_type\": match.ruleId,\n",
        "                        \"message\": match.message,\n",
        "                        \"context\": match.context,\n",
        "                        \"suggestion\": match.replacements[0] if match.replacements else \"No suggestion available\",\n",
        "                        \"position\": (match.offset, match.offset + match.errorLength),\n",
        "                        \"text\": answer_text[match.offset:match.offset + match.errorLength]\n",
        "                    }\n",
        "                    errors_found.append(error_info)\n",
        "\n",
        "            # Store results for this part\n",
        "            all_results[part_number] = {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer_text,\n",
        "                \"errors\": errors_found\n",
        "            }\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@app.post(\"/process_transcription/\")\n",
        "def process_transcription(data: TranscriptionInput):\n",
        "    transcription = data.transcription\n",
        "    transcription_str = '\\n'.join(transcription)\n",
        "    parsed_transcription = parse_transcription(transcription_str)\n",
        "\n",
        "    relevance_result = analyze_answer_relevance(parsed_transcription)\n",
        "    analysis_result = analyze_all_answers(parsed_transcription)\n",
        "\n",
        "    return {\"relevance\": relevance_result, \"analysis\": analysis_result}\n",
        "\n",
        "@app.get(\"/public_url/\")\n",
        "def get_public_url():\n",
        "    # Set your ngrok API token here\n",
        "    ngrok.set_auth_token(\"your_ngrok_api_token\")\n",
        "\n",
        "    # Create a tunnel for your application\n",
        "    url = ngrok.connect(8000).public_url\n",
        "\n",
        "    return {\"public_url\": url}\n",
        "\n",
        "# Public API bằng ngrok\n",
        "if __name__ == \"__main__\":\n",
        "    # Set your ngrok API token here\n",
        "    ngrok.set_auth_token(\"2sWjZTSTemDeBnksDgYJvraqVku_7aYAKKMyNoNhKzoUxhHFU\")\n",
        "\n",
        "    # Create a tunnel for your application\n",
        "    url = ngrok.connect(8000).public_url\n",
        "    print(f\"Public URL: {url}\")\n",
        "\n",
        "    # Run the FastAPI application\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "wofgSRZpQklz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0d289c-dbc0-4f54-fd7f-862638b797d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://4621-34-73-38-44.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1047]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-38' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     42.116.198.240:0 - \"POST /process_transcription/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1047]\n"
          ]
        }
      ]
    }
  ]
}